{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "Place grades for all questions on the same scale (by fitting a gaussian distribution to questions with large range of scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dfea.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "question = df[df.author_censor_id == \"3B9EF7F8-A6DD-46DD-91C0-BAF4182BE288\"][df.question_id == 41702]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question.score_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = question[\"score_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one censor and plot histograms for each question. There are multiple score with one question_id. Just to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "censor = df[df.author_censor_id == \"D164F89A-486D-49FE-96BF-FAC7DE74E4C7\"]\n",
    "for qid in censor.question_id.unique():\n",
    "    censor[censor.question_id == qid].score_value.hist()\n",
    "    print(qid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use RobustScaler for one question and plot several histograms: just raw data and and then scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [12, 4]\n",
    "\n",
    "question = df[df.question_id == 41701]\n",
    "for cid in question.author_censor_id.unique(): # Kaikki sensorit yksitellen: tehdään hist.\n",
    "    print(\"=====================\")\n",
    "    print(cid) # censor_id\n",
    "\n",
    "    grades = question[question.author_censor_id == cid].score_value.hist(bins=100)\n",
    "    plt.show()\n",
    "    \n",
    "    grades = question[question.author_censor_id == cid].score_value.apply(lambda x: -np.log(101-x)).hist(bins=100)\n",
    "    plt.show()\n",
    "    \n",
    "    grades = question[question.author_censor_id == cid].score_value.apply(lambda x: -np.log(101-x))\n",
    "    grades_scaled = RobustScaler().fit_transform(pd.DataFrame(grades))\n",
    "    pd.Series(grades_scaled[:,0]).hist(bins=100)\n",
    "    plt.xlim([-5,5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take just one censor´s one question and scale scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need {studentid: value} for each combination of (qid, censorid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qid = 41701 # question id\n",
    "censorid = \"D164F89A-486D-49FE-96BF-FAC7DE74E4C7\" # censor id\n",
    "\n",
    "question = df[df.question_id == 41701] # take just one question\n",
    "grades = question[question.author_censor_id == censorid].score_value.apply(lambda x: -np.log(101-x)) # one censor and log\n",
    "\n",
    "rsc = RobustScaler().fit(pd.DataFrame(grades)) # fitataan.\n",
    "\n",
    "grades_scaled = grades.apply(lambda x: (x - rsc.center_[0]) / rsc.scale_[0] ) # mean to zero\n",
    "\n",
    "grades_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Build a system to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_student = df.student_uuid.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_plots = True\n",
    "rsc = RobustScaler()\n",
    "students = defaultdict(list) # returns empty list instead of error if key not found.\n",
    "last_cid = -1\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [10, 3]\n",
    "\n",
    "# Part is groupby-chunck for cid, qid i.e. partial dataframe grouped.\n",
    "for (cid, qid), part in df.groupby((df.author_censor_id, df.question_id)):\n",
    "    \n",
    "    # show progress\n",
    "    if last_cid != cid:\n",
    "        last_cid = cid\n",
    "        print(cid)\n",
    "    \n",
    "    scores = part.score_value\n",
    "    max_score = part.score_value.max()\n",
    "    scale = max_score\n",
    "    \n",
    "    if max_score > 10:  # it's a large question with 0-100 scores\n",
    "        scores = scores.apply(lambda x: -np.log(101-x))\n",
    "        scale = max_score / 3  # max scale is 3 sigma, meaning 0.3% best students get 100 points\n",
    "    \n",
    "    # fit a Gaussian to scores\n",
    "    rsc.fit(scores.to_frame())\n",
    "###########################################################################\n",
    "# multiplication by max_score is our guess, not sure how we should use it #\n",
    "#    scores = scores.apply(lambda x: ((x - rsc.center_[0]) / rsc.scale_[0]) * scale)  # max_score for importance weight\n",
    "    scores = scores.apply(lambda x: (x - rsc.center_[0]))  # only remove bias\n",
    "###########################################################################\n",
    "    \n",
    "    # add new scores to student records\n",
    "    for i,v in scores.items():\n",
    "        students[index_to_student[i]].append(v)\n",
    "\n",
    "    if show_plots:\n",
    "        scores.hist(bins=50)\n",
    "        plt.title(\"{}:  {}\".format(qid, cid))\n",
    "        print(qid, cid)\n",
    "        #plt.xlim([-4,4])\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get averages to students_scores based on the dict of lists in students\n",
    "student_scores = {k: np.mean(v) for k,v in students.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(student_scores).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a graph and a linear system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df.student_uuid.unique()\n",
    "students = dict(zip(ids, range(len(ids))))\n",
    "\n",
    "index_to_student_number = df.student_uuid.map(students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "equations = []\n",
    "last_cid = -1\n",
    "\n",
    "for (cid, qid), part in df.groupby((df.author_censor_id, df.question_id)):\n",
    "    \n",
    "    # show progress\n",
    "    if last_cid != cid:\n",
    "        last_cid = cid\n",
    "        print(cid)\n",
    "    \n",
    "    scores = part.score_value\n",
    "    max_score = scores.max()\n",
    "    scores = scores.to_frame().join(index_to_student_number.to_frame())\n",
    "    \n",
    "    for p in itertools.combinations(scores.iterrows(), 2):\n",
    "        if np.random.rand() < 0.01:\n",
    "            st1, st2, val = p[0][1].student_uuid, p[1][1].student_uuid, p[1][1].score_value - p[0][1].score_value\n",
    "            equations.append( (st1, st2, val, max_score) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print(equations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4000  # number of students to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(index_to_student_number.max())\n",
    "np.random.shuffle(idx)\n",
    "random_students = set(idx[:k])\n",
    "random_student_index = dict(zip(list(random_students), range(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_equations = [e for e in equations if e[0] in random_students and e[1] in random_students]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(random_equations)\n",
    "print(n)\n",
    "X = np.zeros((n, k))\n",
    "Y = np.zeros((n, ))\n",
    "\n",
    "for i,(s1,s2,val,w) in enumerate(random_equations):\n",
    "    X[i,random_student_index[s2]] = w   # student B\n",
    "    X[i,random_student_index[s1]] = -w  # - student A\n",
    "    Y[i] = val                          # = gradeB - gradeA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.linalg.lstsq(X, Y, rcond=0.0001)[0]\n",
    "scores.min(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(scores).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"idx\": range(k), \"scores\": [int(s*100) for s in scores]})\n",
    "data.idx = data.idx.map({v:k for k,v in random_student_index.items()})\n",
    "data[\"uuid\"] = data.idx.map({v:k for k,v in students.items()})\n",
    "data = data.drop([\"idx\"], axis=1)\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for alpha in np.logspace(-7, 5, base=10, num=13):\n",
    "    scores = np.linalg.lstsq(X, Y, rcond=alpha)[0]\n",
    "    pd.Series(scores).hist(bins=100)\n",
    "    plt.title(\"alpha = {}\".format(alpha))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "- true grades of students\n",
    "- predict grades of students\n",
    "- sort students by scores \n",
    "- compare bins\n",
    "- compare with school\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
